{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff872904-f029-497b-8b41-c5f801ce6fbc",
   "metadata": {},
   "source": [
    "# Lab 6 -- Decision Trees for Regression\n",
    "\n",
    "Compared to last week, this is a very simple lab <span style=\"font-size:20pt;\">ðŸ˜ƒ</span> You'll have fun programming!\n",
    "\n",
    "You will implement the **Classification and Regression Tree (CART)** algorithm from scratch.\n",
    "\n",
    "The lab is broken down into the following pieces:\n",
    "\n",
    "* Regression Criterion\n",
    "* Creating Splits\n",
    "* Buiding a Tree\n",
    "* Making a prediction\n",
    "\n",
    "## Exercise 1 -- Download and load the dataset\n",
    "\n",
    "We will be using the usual Boston Housing dataset, which is available to download from ECLASS\n",
    "\n",
    "* Download the file\n",
    "* Read it and separate the target variable from the features.\n",
    "* Make a 80/10/10 train/validation/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2285bb42-350c-4042-9f9c-e1a936ca7ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a1f1e4a-feb8-4c43-bcf9-e7d86be427fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "data = pd.read_csv(\"BostonHousing.csv\")\n",
    "y = data[\"medv\"].to_numpy()\n",
    "X = data.drop(\"medv\", axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c7cae-f7f1-4b42-9326-aefec0898cf9",
   "metadata": {},
   "source": [
    "The target variable will be as usual `MEDV`. Use the rest as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5351f2fe-31f7-4c94-97e3-a51f17fcb800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b74cb6-7ffb-4373-8df6-afca9358f635",
   "metadata": {},
   "source": [
    "## Exercise 2 -- Optimization Criterion\n",
    "\n",
    "For regression, a simple criterion to optimize is to minimize the sum of squared errors for a given region. This is, for all datapoints in a region with size, we minimize:\n",
    "\n",
    "$$\\sum_{i=1}^N(y_i - \\hat{y})^2$$\n",
    "\n",
    "where $N$ is the number of datapoits in the region and $\\hat{y}$ is the mean value of the region for the target variable. \n",
    "\n",
    "Implement such a function using the description below.\n",
    "\n",
    "Please, don't use an existing implementation, refer to the [book](https://www.statlearning.com/s/ISLRSeventhPrinting.pdf), and if you need help, ask questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef07acad-8c45-4750-a12d-81ef01966d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def regression_criterion(region):\n",
    "    \"\"\"\n",
    "    Implements the sum of squared error criterion in a region\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    region : ndarray\n",
    "        Array of shape (N,) containing the values of the target values \n",
    "        for N datapoints in the training set.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The sum of squared error\n",
    "        \n",
    "    Note\n",
    "    ----\n",
    "    The error for an empty region should be infinity (use: float(\"inf\"))\n",
    "    This avoids creating empty regions\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(region) == 0:\n",
    "        return float(\"inf\")\n",
    "    \n",
    "    mean_value = np.mean(region)\n",
    "    sse = np.sum((region - mean_value) ** 2)\n",
    "    \n",
    "    return sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67f97c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6200679838629544\n",
      "0.0\n",
      "0.0\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "rng = np.random.default_rng(0)\n",
    "print(regression_criterion(rng.random(size=40)))\n",
    "print(regression_criterion(np.ones(10)))\n",
    "print(regression_criterion(np.zeros(10)))\n",
    "print(regression_criterion(np.array([])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ee5c1-a1b9-4507-bd7b-2e94936ee1e6",
   "metadata": {},
   "source": [
    "## Exercise 3 -- Make a split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cb0cfd2-e271-4355-8953-7cd438e75340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_region(region, feature_index, tau):\n",
    "    \"\"\"\n",
    "    Given a region, splits it based on the feature indicated by\n",
    "    `feature_index`, the region will be split in two, where\n",
    "    one side will contain all points with the feature with values \n",
    "    lower than `tau`, and the other split will contain the \n",
    "    remaining datapoints.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    region : array of size (n_samples, n_features)\n",
    "        a partition of the dataset (or the full dataset) to be split\n",
    "    feature_index : int\n",
    "        the index of the feature (column of the region array) used to make this partition\n",
    "    tau : float\n",
    "        The threshold used to make this partition\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    left_partition : array\n",
    "        indices of the datapoints in `region` where feature < `tau`\n",
    "    right_partition : array\n",
    "        indices of the datapoints in `region` where feature >= `tau` \n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    left_partition = np.where([region[:, feature_index] < tau])[1]\n",
    "    right_partition = np.where([region[:, feature_index] >= tau])[1]\n",
    "\n",
    "    return left_partition, right_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0655f8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224,) (180,)\n"
     ]
    }
   ],
   "source": [
    "l, r = split_region(X_train, 2, 10)\n",
    "print(l.shape, r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9359a-b601-4522-8731-6393500bda73",
   "metadata": {},
   "source": [
    "## Exercise 4 -- Find the best split\n",
    "\n",
    "The strategy is quite simple (as well as inefficient), but it helps to reinforce the concepts.\n",
    "We are going to use a greedy, exhaustive algorithm to select splits, selecting the `feature_index` and the `tau` that minimizes the Regression Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "079c4a77-27ba-44b8-9ad4-b98c80c9cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(X, y):\n",
    "    \"\"\"\n",
    "    Given a dataset (full or partial), splits it on the feature of that minimizes the sum of squared error\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array (n_samples, n_features)\n",
    "        features \n",
    "    y : array (n_samples, )\n",
    "        labels\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    decision : dictionary\n",
    "        keys are:\n",
    "        * 'feature_index' -> an integer that indicates the feature (column) of `X` on which the data is split\n",
    "        * 'tau' -> the threshold used to make the split\n",
    "        * 'left_region' -> array of indices where the `feature_index`th feature of X is lower than `tau`\n",
    "        * 'right_region' -> indices not in `low_region`\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    min_sse = np.inf\n",
    "    min_feature = 0\n",
    "    min_tau = 0\n",
    "    for each_feature in range(X.shape[1]):\n",
    "        for each_tau in np.unique(X[:, each_feature]):\n",
    "            left, right = split_region(X, each_feature, each_tau)\n",
    "            sse1 = regression_criterion(y[left])\n",
    "            sse2 = regression_criterion(y[right])\n",
    "            total_sse = sse1 + sse2\n",
    "\n",
    "            if total_sse < min_sse:\n",
    "                min_sse = total_sse\n",
    "                min_feature = each_feature\n",
    "                min_tau = each_tau\n",
    "                min_left = left\n",
    "                min_right = right\n",
    "\n",
    "    return {\"feature_index\":min_feature, \"tau\":min_tau, \"left_region\":min_left, \"right_region\":min_right}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3583086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Root:\n",
      "{'feature_index': 5, 'tau': 6.812, 'left_region': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 14], dtype=int64), 'right_region': array([12, 13], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "node_root = get_split(X_train[:15, :], y_train[:15])\n",
    "print(\"Node Root:\")\n",
    "print(node_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfbfede4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Left 1:\n",
      "{'feature_index': 6, 'tau': 59.6, 'left_region': array([ 0,  2,  3,  4,  6, 10, 12], dtype=int64), 'right_region': array([ 1,  5,  7,  8,  9, 11], dtype=int64)}\n",
      "Node Right 1:\n",
      "{'feature_index': 0, 'tau': 1.83377, 'left_region': array([0], dtype=int64), 'right_region': array([1], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "node_left = get_split(X_train[node_root[\"left_region\"], :], y_train[node_root[\"left_region\"]])\n",
    "print(\"Node Left 1:\")\n",
    "print(node_left)\n",
    "node_right = get_split(X_train[node_root[\"right_region\"], :], y_train[node_root[\"right_region\"]])\n",
    "print(\"Node Right 1:\")\n",
    "print(node_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06361492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Left Left 2:\n",
      "{'feature_index': 5, 'tau': 6.312, 'left_region': array([0, 3, 4, 6], dtype=int64), 'right_region': array([1, 2, 5], dtype=int64)}\n",
      "Node Left Right 2:\n",
      "{'feature_index': 12, 'tau': 18.13, 'left_region': array([1, 3, 4, 5], dtype=int64), 'right_region': array([0, 2], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "node_left_left = get_split(X_train[node_root[\"left_region\"], :][node_left[\"left_region\"], :], y_train[node_root[\"left_region\"]][node_left[\"left_region\"]])\n",
    "print(\"Node Left Left 2:\")\n",
    "print(node_left_left)\n",
    "node_left_right = get_split(X_train[node_root[\"left_region\"], :][node_left[\"right_region\"], :], y_train[node_root[\"left_region\"]][node_left[\"right_region\"]])\n",
    "print(\"Node Left Right 2:\")\n",
    "print(node_left_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8797ab0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Left Left Left 3:\n",
      "{'feature_index': 0, 'tau': 0.79041, 'left_region': array([0, 1, 2], dtype=int64), 'right_region': array([3], dtype=int64)}\n",
      "Node Left Right Left 3:\n",
      "{'feature_index': 0, 'tau': 5.69175, 'left_region': array([1, 2, 3], dtype=int64), 'right_region': array([0], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "node_left_left_left = get_split(X_train[node_root[\"left_region\"], :][node_left[\"left_region\"], :][node_left_left[\"left_region\"], :], y_train[node_root[\"left_region\"]][node_left[\"left_region\"]][node_left_left[\"left_region\"]])\n",
    "print(\"Node Left Left Left 3:\")\n",
    "print(node_left_left_left)\n",
    "\n",
    "node_left_right_left = get_split(X_train[node_root[\"left_region\"], :][node_left[\"right_region\"], :][node_left_right[\"left_region\"], :], y_train[node_root[\"left_region\"]][node_left[\"right_region\"]][node_left_right[\"left_region\"]])\n",
    "print(\"Node Left Right Left 3:\")\n",
    "print(node_left_right_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d096e1f-fddf-4018-8ea9-252cb21de8c7",
   "metadata": {},
   "source": [
    "## Exercise 5 -- Recursive Splitting\n",
    "\n",
    "The test above is an example on how to find the root node of our decision tree. The algorithm now is a greedy search until we reach a stop criterion. To find the actual root node of our decision tree, you must provide the whole training set, not just a slice of 15 rows as the test above.\n",
    "\n",
    "The trivial stopping criterion is to recursively grow the tree until each split contains a single point (perfect node purity). If we go that far, it normally means we are overfitting.\n",
    "\n",
    "You will implement these criteria to stop the growth:\n",
    "\n",
    "* A node is a leaf if:\n",
    "    * It has less than `min_samples` datapoints\n",
    "    * It is at the `max_depth` level from the root (each split creates a new level)\n",
    "    * The criterion is `0`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "88f6193d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_index': 12,\n",
       " 'tau': 18.13,\n",
       " 'left_region': {'feature_index': 0,\n",
       "  'tau': 5.69175,\n",
       "  'left_region': array([1, 2, 3], dtype=int64),\n",
       "  'right_region': array([0], dtype=int64)},\n",
       " 'right_region': 1.0}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = get_split(X_train[node_root[\"left_region\"], :][node_left[\"right_region\"], :], y_train[node_root[\"left_region\"]][node_left[\"right_region\"]])\n",
    "\n",
    "X = X_train[node_root[\"left_region\"], :][node_left[\"right_region\"], :]\n",
    "y = y_train[node_root[\"left_region\"]][node_left[\"right_region\"]]\n",
    "\n",
    "if len(node[\"right_region\"]) <= 3:\n",
    "    node[\"right_region\"] = np.mean(node[\"right_region\"])\n",
    "else:\n",
    "    node[\"right_region\"] = get_split(X[node[\"right_region\"], :], y[node[\"right_region\"]])\n",
    "\n",
    "if len(node[\"left_region\"]) <= 3:\n",
    "    node[\"left_region\"] = np.mean(node[\"left_region\"])\n",
    "else:\n",
    "    node[\"left_region\"] = get_split(X[node[\"left_region\"], :], y[node[\"left_region\"]])\n",
    "node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "89e826c6-cb8b-488c-9eb4-d4b6f2d4e9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_growth(node, min_samples, max_depth, current_depth, X, y):\n",
    "    \"\"\"\n",
    "    Recursively grows a decision tree.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    node : dictionary\n",
    "        If the node is terminal, it contains only the \"value\" key, which determines the value to be used as a prediction.\n",
    "        If the node is not terminal, the dictionary has the structure defined by `get_split`\n",
    "    min_samples : int\n",
    "        parameter for stopping criterion if a node has <= min_samples datapoints\n",
    "    max_depth : int\n",
    "        parameter for stopping criterion if a node belongs to this depth\n",
    "    depth : int\n",
    "        current distance from the root\n",
    "    X : array (n_samples, n_features)\n",
    "        features (full dataset)\n",
    "    y : array (n_samples, )\n",
    "        labels (full dataset)\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    To create a terminal node, a dictionary is created with a single \"value\" key, with a value that\n",
    "    is the mean of the target variable\n",
    "    \n",
    "    'left' and 'right' keys are added to non-terminal nodes, which contain (possibly terminal) nodes \n",
    "    from higher levels of the tree:\n",
    "    'left' corresponds to the 'left_region' key, and 'right' to the 'right_region' key\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "    \n",
    "    # Left Region\n",
    "    # jÃ¡ Ã© a o valor cerinho\n",
    "    print(f\"Currendt Depth: {current_depth}\")\n",
    "    print(node)\n",
    "    print()\n",
    "    if len(node[\"left_region\"]) == 1:\n",
    "        pass\n",
    "    elif len(node[\"left_region\"]) <= min_samples or current_depth >= max_depth:\n",
    "        node[\"left\"] = {\"value\":np.mean(y[node[\"left_region\"]])}\n",
    "        print(f\"Currendt Depth: {current_depth}\", \"Left:\", node[\"left\"])\n",
    "    else:\n",
    "        X_l = X[node[\"left_region\"], :]\n",
    "        y_l = y[node[\"left_region\"]]\n",
    "        node[\"left_region\"] = get_split(X[node[\"left_region\"], :], y[node[\"left_region\"]])\n",
    "        recursive_growth(node[\"left_region\"], min_samples, max_depth, current_depth + 1, X_l, y_l)\n",
    "\n",
    "    # Right Region\n",
    "    if len(node[\"right_region\"]) == 1:\n",
    "        pass\n",
    "    elif len(node[\"right_region\"]) <= min_samples or current_depth >= max_depth:\n",
    "        node[\"right\"] = {\"value\":np.mean(y[node[\"right_region\"]])}\n",
    "        print(f\"Currendt Depth: {current_depth}\", \"Right:\", node[\"right\"])\n",
    "    else:\n",
    "        X_r = X[node[\"right_region\"], :]\n",
    "        y_r = y[node[\"right_region\"]]\n",
    "        node[\"right_region\"] = get_split(X[node[\"right_region\"], :], y[node[\"right_region\"]])\n",
    "        recursive_growth(node[\"right_region\"], min_samples, max_depth, current_depth + 1, X_r, y_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e27714b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currendt Depth: 1\n",
      "{'feature_index': 5, 'tau': 6.812, 'left_region': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 14, 15, 16, 18, 19],\n",
      "      dtype=int64), 'right_region': array([12, 13, 17], dtype=int64)}\n",
      "\n",
      "Currendt Depth: 2\n",
      "{'feature_index': 6, 'tau': 59.6, 'left_region': array([ 0,  2,  3,  4,  6, 10, 12, 14, 15], dtype=int64), 'right_region': array([ 1,  5,  7,  8,  9, 11, 13, 16], dtype=int64)}\n",
      "\n",
      "Currendt Depth: 3\n",
      "{'feature_index': 5, 'tau': 6.326, 'left_region': array([0, 2, 3, 4, 6, 7, 8], dtype=int64), 'right_region': array([1, 5], dtype=int64)}\n",
      "\n",
      "Currendt Depth: 4\n",
      "{'feature_index': 2, 'tau': 7.38, 'left_region': array([0, 2, 3], dtype=int64), 'right_region': array([1, 4, 5, 6], dtype=int64)}\n",
      "\n",
      "Currendt Depth: 4 Left: {'value': 19.23333333333333}\n",
      "Currendt Depth: 4 Right: {'value': 21.825}\n",
      "Currendt Depth: 3 Right: {'value': 26.2}\n",
      "Currendt Depth: 3\n",
      "{'feature_index': 12, 'tau': 18.13, 'left_region': array([1, 3, 4, 5], dtype=int64), 'right_region': array([0, 2, 6, 7], dtype=int64)}\n",
      "\n",
      "Currendt Depth: 3 Left: {'value': 18.55}\n",
      "Currendt Depth: 3 Right: {'value': 15.4}\n",
      "Currendt Depth: 1 Right: {'value': 41.63333333333333}\n"
     ]
    }
   ],
   "source": [
    "k = 20\n",
    "test_root = get_split(X_train[:k, :], y_train[:k])\n",
    "recursive_growth(test_root, 5, 4, 1, X_train[:k, :], y_train[:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "special-alexander",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currendt Depth: 0\n",
      "{'feature_index': 5, 'tau': 6.812, 'left_region': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 14, 15, 16, 18, 19],\n",
      "      dtype=int64), 'right_region': array([12, 13, 17], dtype=int64)}\n",
      "\n",
      "Currendt Depth: 1\n",
      "{'feature_index': 6, 'tau': 59.6, 'left_region': array([ 0,  2,  3,  4,  6, 10, 12, 14, 15], dtype=int64), 'right_region': array([ 1,  5,  7,  8,  9, 11, 13, 16], dtype=int64)}\n",
      "\n",
      "Currendt Depth: 2\n",
      "{'feature_index': 5, 'tau': 6.326, 'left_region': array([0, 2, 3, 4, 6, 7, 8], dtype=int64), 'right_region': array([1, 5], dtype=int64)}\n",
      "\n",
      "Currendt Depth: 3\n",
      "{'feature_index': 2, 'tau': 7.38, 'left_region': array([0, 2, 3], dtype=int64), 'right_region': array([1, 4, 5, 6], dtype=int64)}\n",
      "\n",
      "Left: {'value': 19.23333333333333}\n",
      "Currendt Depth: 4\n",
      "{'feature_index': 0, 'tau': 0.30347, 'left_region': array([2, 3], dtype=int64), 'right_region': array([0, 1], dtype=int64)}\n",
      "\n",
      "Left: {'value': 21.1}\n",
      "Right: {'value': 22.55}\n",
      "Right: {'value': 26.2}\n",
      "Currendt Depth: 2\n",
      "{'feature_index': 12, 'tau': 18.13, 'left_region': array([1, 3, 4, 5], dtype=int64), 'right_region': array([0, 2, 6, 7], dtype=int64)}\n",
      "\n",
      "Currendt Depth: 3\n",
      "{'feature_index': 0, 'tau': 5.69175, 'left_region': array([1, 2, 3], dtype=int64), 'right_region': array([0], dtype=int64)}\n",
      "\n",
      "Left: {'value': 18.366666666666667}\n",
      "Currendt Depth: 3\n",
      "{'feature_index': 0, 'tau': 18.811, 'left_region': array([0, 1, 2], dtype=int64), 'right_region': array([3], dtype=int64)}\n",
      "\n",
      "Left: {'value': 14.566666666666668}\n",
      "Right: {'value': 41.63333333333333}\n"
     ]
    }
   ],
   "source": [
    "# fill in the gaps with your code\n",
    "min_samples = 3\n",
    "max_depth = 6\n",
    "root = get_split(X_train[:20, :], y_train[:20])\n",
    "recursive_growth(root, min_samples, max_depth, 0, X_train[:20, :], y_train[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a32ee99-9103-4586-8555-ccb905acd4cd",
   "metadata": {},
   "source": [
    "Below we provide code to visualise the generated tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8022f376-bff9-4455-9650-8ef66ca772da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, depth):\n",
    "    if 'value' in node.keys():\n",
    "        print('.  '*(depth-1), f\"[{node['value']}]\")\n",
    "    else:\n",
    "        print('.  '*depth, f'X_{node[\"feature_index\"]} < {node[\"tau\"]}')\n",
    "        print_tree(node['left'], depth+1)\n",
    "        print_tree(node['right'], depth+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a0b4dfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X_5 < 6.812\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'left'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprint_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[94], line 6\u001b[0m, in \u001b[0;36mprint_tree\u001b[1;34m(node, depth)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.  \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39mdepth, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_index\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m < \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtau\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     print_tree(\u001b[43mnode\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, depth\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m     print_tree(node[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m], depth\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'left'"
     ]
    }
   ],
   "source": [
    "print_tree(root, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "natural-assignment",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X_4 < 0.581\n",
      ".   X_10 < 15.9\n",
      ".  .   X_1 < 20.0\n",
      ".  .   [20.616666666666664]\n",
      ".  .   [20.616666666666664]\n",
      ".  .   X_0 < 0.01439\n",
      ".  .   [28.200000000000003]\n",
      ".  .   [28.200000000000003]\n",
      ".   X_7 < 2.5671\n",
      ".  .   X_0 < 9.82349\n",
      ".  .   [22.175193798449612]\n",
      ".  .   [22.175193798449612]\n",
      ".  .   X_0 < 8.71675\n",
      ".  .   [21.292307692307688]\n",
      ".  .   [21.292307692307688]\n"
     ]
    }
   ],
   "source": [
    "print_tree(test_root, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e063e7-58f5-4278-86fb-4096c29a32e7",
   "metadata": {},
   "source": [
    "# Exercise 6 -- Make a Prediction\n",
    "Use the a node to predict the class of a compatible dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb6db1b9-7fb1-4949-8a31-a0eb7364d0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(node, sample):\n",
    "    \"\"\"\n",
    "    Makes a prediction based on the decision tree defined by `node`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    node : dictionary\n",
    "        A node created one of the methods above\n",
    "    sample : array of size (n_features,)\n",
    "        a sample datapoint\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "        \n",
    "def predict(node, X):\n",
    "    \"\"\"\n",
    "    Makes a prediction based on the decision tree defined by `node`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    node : dictionary\n",
    "        A node created one of the methods above\n",
    "    X : array of size (n_samples, n_features)\n",
    "        n_samples predictions will be made\n",
    "    \"\"\"\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0660e974-11cf-4e0a-b402-a8f2ed3978c2",
   "metadata": {},
   "source": [
    "Now use the functions defined above to calculate the RMSE of the validation set. \n",
    "* Try first with `min_samples=20` and `max_depth=6` (for this values you should get a validation RMSE of ~8.8)\n",
    "\n",
    "Then, experiment with different values for the stopping criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9526e9-b82f-4783-b7e0-762fc87d68bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
